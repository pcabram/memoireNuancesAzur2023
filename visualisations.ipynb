{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec.load(\"models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 500\n",
    "common_words = model.wv.index_to_key[:top_n]\n",
    "common_vectors = model.wv[common_words]\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=100)\n",
    "word_vectors_2d = tsne.fit_transform(common_vectors)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], s=10)\n",
    "for i, word in enumerate(common_words):\n",
    "    plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "word_vectors_2d = tsne.fit_transform(common_vectors)\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x=word_vectors_2d[:, 0],\n",
    "    y=word_vectors_2d[:, 1],\n",
    "    mode='markers',\n",
    "    text=common_words\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"Word2Vec t-SNE Visualization\",\n",
    "    hovermode='closest',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "word_vectors = model.wv.vectors\n",
    "\n",
    "kmeans = KMeans(n_clusters=50)\n",
    "labels = kmeans.fit_predict(word_vectors)\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x=word_vectors_2d[:, 0],\n",
    "    y=word_vectors_2d[:, 1],\n",
    "    mode='markers',\n",
    "    text=common_words,\n",
    "    marker=dict(\n",
    "        color=labels,\n",
    "        colorscale='Viridis',\n",
    "        size=8,\n",
    "        line_width=1\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"Word2Vec t-SNE Visualization with K-means Clusters\",\n",
    "    hovermode='closest',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "for word, label in zip(common_words, labels):\n",
    "    if label not in clusters:\n",
    "        clusters[label] = []\n",
    "    clusters[label].append(word)\n",
    "\n",
    "# Print out the words in each cluster\n",
    "for label, words in clusters.items():\n",
    "    print(f'Cluster {label}: {words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=50)\n",
    "labels = kmeans.fit_predict(common_vectors)\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x=word_vectors_2d[:, 0],\n",
    "    y=word_vectors_2d[:, 1],\n",
    "    mode='markers',\n",
    "    text=common_words,\n",
    "    marker=dict(\n",
    "        color=labels,\n",
    "        colorscale='Viridis',\n",
    "        size=8,\n",
    "        line_width=1\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"Word2Vec t-SNE Visualization with K-means Clusters\",\n",
    "    hovermode='closest',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "x_start, x_end = 4, 6.4 \n",
    "y_start, y_end = 0.65, 8\n",
    "\n",
    "indices = (word_vectors_2d[:, 0] >= x_start) & (word_vectors_2d[:, 0] <= x_end) & (word_vectors_2d[:, 1] >= y_start) & (word_vectors_2d[:, 1] <= y_end)\n",
    "region_vectors = word_vectors_2d[indices]\n",
    "region_labels = [common_words[i] for i in range(len(common_words)) if indices[i]]\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x=region_vectors[:, 0],\n",
    "    y=region_vectors[:, 1],\n",
    "    mode='markers+text',\n",
    "    text=region_labels,\n",
    "    textposition=\"bottom center\"\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"Word2Vec t-SNE Visualization\",\n",
    "    hovermode='closest',\n",
    "    xaxis=dict(range=[x_start, x_end]),\n",
    "    yaxis=dict(range=[y_start, y_end])\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = \"francés\"\n",
    "word2 = \"italiano\"\n",
    "similarity_score = model.wv.similarity(word1, word2)\n",
    "\n",
    "print(f\"Similarity between '{word1}' and '{word2}': {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar(['francés', 'francia'], topn=30)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar(['mexicano', 'méxico'], topn=30)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar(['literatura', 'escritor', 'poema', 'poesía', 'novela', 'libro'], topn=30)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar('mexicano', topn=30)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar('italiano', topn=30)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar('cubano', topn=30)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar('chino', topn=30)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar('moda', topn=30)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar('obrero', topn=30)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar('artista', topn=30)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar('', topn=30)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# Load the model\n",
    "doc2vec_model = Doc2Vec.load(\"models/doc2vec.model\")\n",
    "\n",
    "doc_vectors = doc2vec_model.dv.vectors\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=50)\n",
    "doc_vectors_2d = tsne.fit_transform(doc_vectors)\n",
    "\n",
    "kmeans = KMeans(n_clusters=20)\n",
    "labels = kmeans.fit_predict(doc_vectors)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(doc_vectors_2d[:, 0], doc_vectors_2d[:, 1], c=labels)\n",
    "\n",
    "handles, _ = scatter.legend_elements(prop='colors')\n",
    "plt.legend(handles, [f'Cluster {i+1}' for i in range(20)], loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('preprocessing_data/tagged_documents.pkl', 'rb') as f:\n",
    "    tagged_documents = pickle.load(f)\n",
    "\n",
    "clusters = [[] for _ in range(20)]\n",
    "\n",
    "for label, doc_tag in enumerate(labels):\n",
    "    clusters[doc_tag].append(label)\n",
    "\n",
    "cluster_documents = [[tagged_documents[doc_tag].words[:15] for doc_tag in cluster] for cluster in clusters]\n",
    "\n",
    "cluster_documents = [[(' '.join(doc)) for doc in cluster] for cluster in cluster_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "combined_vectors = np.load(\"models/combined_vectors.npy\")\n",
    "\n",
    "# Use t-SNE to reduce the combined vectors to two dimensions\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=50)\n",
    "combined_vectors_2d = tsne.fit_transform(combined_vectors)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(combined_vectors_2d[:, 0], combined_vectors_2d[:, 1])\n",
    "plt.title(\"Combined Vectors t-SNE Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def find_closest_documents(target_document_index, num_closest=5):\n",
    "    combined_vectors = np.load(\"models/combined_vectors.npy\")\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity=50)\n",
    "    combined_vectors_2d = tsne.fit_transform(combined_vectors)\n",
    "\n",
    "    # Calculate the distances between the target document and all other documents\n",
    "    target_vector = combined_vectors_2d[target_document_index]\n",
    "    distances = [distance.euclidean(target_vector, vector) for vector in combined_vectors_2d]\n",
    "\n",
    "    closest_indices = np.argsort(distances)[:num_closest]\n",
    "\n",
    "    return closest_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "combined_vectors = np.load(\"models/combined_vectors.npy\")\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=50)\n",
    "combined_vectors_2d = tsne.fit_transform(combined_vectors)\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans(n_clusters=20)\n",
    "labels = kmeans.fit_predict(doc_vectors)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(combined_vectors_2d[:, 0], combined_vectors_2d[:, 1], c=labels)\n",
    "plt.title(\"Combined Vectors t-SNE Visualization\")\n",
    "\n",
    "handles, _ = scatter.legend_elements(prop='colors')\n",
    "plt.legend(handles, [f'Cluster {i+1}' for i in range(20)], loc='upper right')  # moved legend\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def find_closest_documents(target_document_index, num_closest=5):\n",
    "    combined_vectors = np.load(\"models/combined_vectors.npy\")\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity=50)\n",
    "    combined_vectors_2d = tsne.fit_transform(combined_vectors)\n",
    "\n",
    "    target_vector = combined_vectors_2d[target_document_index]\n",
    "    distances = [distance.euclidean(target_vector, vector) for vector in combined_vectors_2d]\n",
    "\n",
    "    closest_indices = np.argsort(distances)[:num_closest]\n",
    "\n",
    "    return closest_indices\n",
    "\n",
    "# Call the function and specify the index of the target document\n",
    "target_document_index = 0\n",
    "num_closest = 5\n",
    "\n",
    "closest_indices = find_closest_documents(target_document_index, num_closest)\n",
    "print(\"Closest document indices:\", closest_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "reduced_vectors = tsne.fit_transform(combined_vectors)\n",
    "\n",
    "# Get the indices of the top 10 most similar documents\n",
    "top10_indices = np.argsort(similarities[0])[-10:]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], color='blue', alpha=0.5)\n",
    "plt.scatter(reduced_vectors[top10_indices, 0], reduced_vectors[top10_indices, 1], color='red', alpha=0.5)\n",
    "plt.scatter(reduced_vectors[most_similar_index, 0], reduced_vectors[most_similar_index, 1], color='green', alpha=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "with open('preprocessing_data/tagged_documents.pkl', 'rb') as f:\n",
    "    tagged_documents = pickle.load(f)\n",
    "\n",
    "lda_model = LdaModel.load('models/lda_model')\n",
    "\n",
    "dictionary = Dictionary.load('models/dictionary')\n",
    "\n",
    "top10_indices = np.argsort(similarities[0])[-10:]\n",
    "\n",
    "for idx in top10_indices:\n",
    "    print(f\"Document index: {idx}, similarity: {similarities[0][idx]}\")\n",
    "    print(f\"Document: {tagged_documents[idx].words}\")\n",
    "\n",
    "    bow_corpus = dictionary.doc2bow(tagged_documents[idx].words)\n",
    "\n",
    "    topic_distribution = lda_model.get_document_topics(bow_corpus)\n",
    "\n",
    "    # Print the topic distribution in each document\n",
    "    for topic_id, prop in topic_distribution:\n",
    "        print(f\"Topic ID: {topic_id}, Proportion: {prop}\")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 15 words from each topic\n",
    "for idx, topic in lda_model.print_topics(-1, 15):\n",
    "    print(f\"Topic: {idx} \\nWords: {topic}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import MmCorpus\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Load the saved dictionary, corpus and LDA model\n",
    "dictionary = Dictionary.load('models/dictionary')\n",
    "corpus = MmCorpus('models/corpus.mm')\n",
    "lda_model = LdaModel.load('models/lda_model')\n",
    "\n",
    "lda_display = gensimvis.prepare(lda_model, corpus, dictionary, sort_topics=False)\n",
    "\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_topics_with_word(lda_model, dictionary, word, num_topics):\n",
    "#     word_id = dictionary.token2id.get(word)\n",
    "#     if word_id is not None:\n",
    "#         word_topics = lda_model.get_term_topics(word_id, minimum_probability=0.000001)\n",
    "#         word_topics.sort(key=lambda x: x[1], reverse=True)\n",
    "#         for topic_id, relevance in word_topics[:num_topics]:\n",
    "#             print(f\"Topic {topic_id}, relevance: {relevance}\")\n",
    "#             print(lda_model.show_topic(topic_id))\n",
    "#     else:\n",
    "#         print(f\"The word '{word}' is not in the dictionary.\")\n",
    "\n",
    "# lda_model, dictionary, _ = topic_modeling_pipeline(lower_texts_words)\n",
    "# find_topics_with_word(lda_model, dictionary, 'francia', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False, num_topics=15)\n",
    "\n",
    "# Create a WordCloud for each topic\n",
    "for idx, topic in topics:\n",
    "    wc = WordCloud(background_color=\"white\", max_words=20, width=800, height=400)\n",
    "    wordcloud = wc.generate_from_frequencies(dict(topic))\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Topic #\" + str(idx))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def document_topic_distribution(lda_model, corpus):\n",
    "    # get the topic distribution for each document\n",
    "    doc_topic_dist = [lda_model.get_document_topics(bow) for bow in corpus]\n",
    "    \n",
    "    # initialize a dataframe\n",
    "    dist_df = pd.DataFrame.from_records([{v:0 for v in range(lda_model.num_topics)}])\n",
    "\n",
    "    # populate the dataframe\n",
    "    for i in doc_topic_dist:\n",
    "        for j in i:\n",
    "            dist_df.loc[0, j[0]] += 1\n",
    "    \n",
    "    # normalize the distribution (optional)\n",
    "    dist_df = dist_df.div(dist_df.sum(axis=1), axis=0)\n",
    "    \n",
    "    return dist_df\n",
    "\n",
    "# get the topic distribution\n",
    "dist_df = document_topic_distribution(lda_model, corpus)\n",
    "\n",
    "# plot the distribution\n",
    "dist_df.transpose().plot(kind='bar', legend=False)\n",
    "plt.title('Topic Distribution across all documents')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Topic ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(lda_model, num_topics, num_words=20):\n",
    "    top_words = [[word for word,_ in lda_model.show_topic(topic_id, topn=50)] for topic_id in range(lda_model.num_topics)]\n",
    "    top_betas = [[beta for _,beta in lda_model.show_topic(topic_id, topn=50)] for topic_id in range(lda_model.num_topics)]\n",
    "\n",
    "    for i in range(num_topics):\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.barh(range(num_words), top_betas[i][:num_words], align='center',color='blue', ecolor='black')\n",
    "        plt.yticks(range(num_words), top_words[i][:num_words])\n",
    "        plt.xlabel('Beta')\n",
    "        plt.title('Top Words in Topic ' + str(i) + ' and their Betas')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "\n",
    "# plot the top words\n",
    "plot_top_words(lda_model, 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lattice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
