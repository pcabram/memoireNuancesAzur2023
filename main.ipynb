{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "FOLDER_PATH = '/xxx/preprocessing_data'\n",
    "\n",
    "def open_txt_file(filename):\n",
    "    file_path = os.path.join(FOLDER_PATH, filename)\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "def clean_and_save(text, filename, cleaners):\n",
    "    cleaned_text = text\n",
    "    for cleaner in cleaners:\n",
    "        cleaned_text = cleaner(cleaned_text)\n",
    "    \n",
    "    new_filepath = os.path.join(FOLDER_PATH, f'{filename[:-4]}Clean.txt')\n",
    "    \n",
    "    with open(new_filepath, 'w') as f:\n",
    "        f.write(cleaned_text)\n",
    "        \n",
    "    print(f\"Cleaned text saved to {new_filepath}!\")\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_hyphen(text):\n",
    "    return re.sub(r'-\\n', '', text)\n",
    "\n",
    "def remove_revista_azul(text):\n",
    "    return re.sub(r'(\\n\\d+\\n[Rr][Ee][Vv][Ii][Ss][Tt][Aa] [Aa][Zz][Uu][Ll]|\\n[Rr][Ee][Vv][Ii][Ss][Tt][Aa] [Aa][Zz][Uu][Ll]\\n\\d+)\\n', '\\n', text)\n",
    "\n",
    "def remove_page_headers(text):\n",
    "    text = re.sub(r'\f\\n\\n## p\\. \\d+ \\(#\\d+\\) ##+\\n\\n---', '', text)\n",
    "    text = re.sub(r'## p\\. \\d+ \\(#\\d+\\) ##+', '', text)\n",
    "    return re.sub(r'\f', '', text)\n",
    "\n",
    "def remove_incomplete_page_headers(text):\n",
    "    return re.sub(r'## p\\. \\(#\\d+\\) ##+', '', text)\n",
    "\n",
    "def remove_excessive_linebreaks(text):\n",
    "    return re.sub(r'\\n{2,}', '\\n', text)\n",
    "\n",
    "def remove_double_linebreaks(text):\n",
    "    return re.sub(r'\\n\\n', '\\n', text)\n",
    "\n",
    "def remove_linebreaks(text):\n",
    "    return re.sub(r'\\n', ' ', text)\n",
    "\n",
    "def remove_all_caps_revista_azul(text):\n",
    "    return re.sub(r'[Rr][Ee][Vv][Ii][Ss][Tt][Aa] [Aa][Zz][Uu][Ll]', '', text, flags=re.I)\n",
    "\n",
    "\n",
    "filenames = ['RevAz1.txt', 'RevAz2.txt']\n",
    "\n",
    "cleaners = [\n",
    "    remove_hyphen,\n",
    "    remove_revista_azul,\n",
    "    remove_page_headers,\n",
    "    remove_incomplete_page_headers,\n",
    "    remove_excessive_linebreaks,\n",
    "    remove_double_linebreaks,\n",
    "    remove_linebreaks,\n",
    "    remove_all_caps_revista_azul,\n",
    "]\n",
    "\n",
    "for filename in filenames:\n",
    "    text = open_txt_file(filename)\n",
    "    clean_and_save(text, filename, cleaners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "with open('/xxx', 'r', encoding='utf-8') as file:\n",
    "    input_text = file.read()\n",
    "\n",
    "max_tokens = 8000\n",
    "\n",
    "chunks = [input_text[i:i+max_tokens] for i in range(0, len(input_text), max_tokens)]\n",
    "\n",
    "openai.api_key = 'xxx'\n",
    "\n",
    "instruction = \"You are a language model assistant. You are asked to correct the spelling mistakes in the text. You are not allowed to add anything new to the text. You're only allowed to correct the spelling mistakes. It's a XIXth century magazine from Mexico. It's mainly in Spanish, but I sometimes has some words in other languages like French and English. You will correct the spelling mistakes in Spanish and leave the words in other languages as they are. If you find the series of caracters '***', don't change it.\"\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": \"N un volante azul que me envía el regente de la imprenta leo estas palabras escritas con lápiz: falta el programa. Calle! Es verdad. Ni mi amigo ni yo pensamos nunca en el programa. Deberoia ir á mi cass por él? ***\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"En un volante azul que me envía el regente de la imprenta leo estas palabras escritas con lápiz: falta el programa. ¡Calle! Es verdad. Ni mi amigo ni yo pensamos nunca en el programa. ¿Debería ir a mi casa por él? ***\"},\n",
    "        {\"role\": \"user\", \"content\": chunk},        \n",
    "    ]\n",
    "    \n",
    "    response = openai.ChatCompletion.create(model='gpt-3.5-turbo-16k', messages=messages, temperature=0.1)\n",
    "\n",
    "    if response is not None:\n",
    "        completion = response['choices'][0]['message']['content'] + \" \"\n",
    "        with open('/xxx/preprocessing_data/RevAzSpellChecked.txt', 'a', encoding='utf-8') as file:\n",
    "            file.write(completion)\n",
    "    else:\n",
    "        print(f'Error: Failed to get response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "\n",
    "def open_txt_file(filename):\n",
    "    folder_path = '/xxx/preprocessing_data'\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def split_into_texts(text):\n",
    "    return text.split('***')\n",
    "\n",
    "def sentencize_texts(texts, nlp):\n",
    "    return [[sent.text for sent in nlp(text).sents] for text in texts]\n",
    "\n",
    "def remove_punctuation(sentencized_texts):\n",
    "    return [[re.sub(r'[^\\w\\s]', '', sentence) for sentence in text] for text in sentencized_texts]\n",
    "\n",
    "def lemmatize_texts(cleaned_texts, nlp):\n",
    "    return [[' '.join([token.lemma_ for token in nlp(sentence)]) for sentence in text] for text in cleaned_texts]\n",
    "\n",
    "def remove_short_and_stop_words(lemmatized_texts):\n",
    "    return [[' '.join([word for word in sentence.split() if len(word) > 2 and word not in STOP_WORDS]) for sentence in text] for text in lemmatized_texts]\n",
    "\n",
    "def sentence_to_words(final_texts):\n",
    "    return [[sentence.split() for sentence in text] for text in final_texts]\n",
    "\n",
    "def convert_to_lowercase(texts_words):\n",
    "    return [[[word.lower() for word in sentence] for sentence in text] for text in texts_words]\n",
    "\n",
    "def preprocess_texts(filename):\n",
    "    # Load language models\n",
    "    sentencizer = spacy.load('xx_sent_ud_sm')\n",
    "    lemmatizer = spacy.load('es_dep_news_trf')\n",
    "\n",
    "    text = open_txt_file(filename)\n",
    "    texts = split_into_texts(text)\n",
    "    sentencized_texts = sentencize_texts(texts, sentencizer)\n",
    "    cleaned_texts = remove_punctuation(sentencized_texts)\n",
    "    lemmatized_texts = lemmatize_texts(cleaned_texts, lemmatizer)\n",
    "    final_texts = remove_short_and_stop_words(lemmatized_texts)\n",
    "    texts_words = sentence_to_words(final_texts)\n",
    "    \n",
    "    return convert_to_lowercase(texts_words)\n",
    "\n",
    "def prepare_for_training(lower_texts_words):\n",
    "    return [sentence for text in lower_texts_words for sentence in text]\n",
    "\n",
    "lower_texts_words = preprocess_texts('RevAzSpellChecked.txt')\n",
    "\n",
    "flattened_texts_words = prepare_for_training(lower_texts_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# def train_word2vec(flattened_texts_words, min_count=1):\n",
    "#     model = Word2Vec(flattened_texts_words, min_count=1)\n",
    "#     model.save(\"models/word2vec.model\")\n",
    "\n",
    "# train_word2vec(flattened_texts_words, min_count=1)\n",
    "\n",
    "def train_word2vec(flattened_texts_words, vector_size=100, window=5, min_count=1, sg=1):\n",
    "    model = Word2Vec(flattened_texts_words, vector_size=vector_size, window=window, min_count=min_count, sg=1)\n",
    "    model.save(\"models/word2vec.model\")\n",
    "\n",
    "train_word2vec(flattened_texts_words, vector_size=100, window=5, min_count=5, sg=1)\n",
    "\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# def train_word2vec(flattened_texts_words, vector_size=10, window=3, min_count=5, workers=4, sg=1, alpha=0.03, min_alpha=0.0007, negative=20):\n",
    "#     model = Word2Vec(flattened_texts_words, vector_size=vector_size, window=window, min_count=min_count, workers=workers, sg=sg, alpha=alpha, min_alpha=min_alpha, negative=negative)\n",
    "#     model.save(\"models/word2vec.model\")\n",
    "\n",
    "# train_word2vec(flattened_texts_words, vector_size=500, window=2, min_count=5, workers=8, sg=1, alpha=0.03, min_alpha=0.0007, negative=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pickle\n",
    "\n",
    "def documents_to_tagged_documents(lower_texts_words):\n",
    "    tagged_documents = [TaggedDocument(words=[word for sentence in text for word in sentence], tags=[i]) for i, text in enumerate(lower_texts_words)]\n",
    "    return tagged_documents\n",
    "\n",
    "tagged_documents = documents_to_tagged_documents(lower_texts_words)\n",
    "\n",
    "def train_doc2vec(tagged_documents, vector_size=150, window=10, min_count=2, workers=8, epochs=20, dm=0, alpha=0.03, min_alpha=0.0007, sample=1e-3):\n",
    "    model = Doc2Vec(vector_size=vector_size, window=window, min_count=min_count, workers=workers, epochs=epochs, dm=dm, alpha=alpha, min_alpha=min_alpha, sample=sample)\n",
    "    model.build_vocab(tagged_documents)\n",
    "    model.train(tagged_documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    model.save(\"models/doc2vec.model\")\n",
    "\n",
    "    with open('preprocessing_data/tagged_documents.pkl', 'wb') as f:\n",
    "        pickle.dump(tagged_documents, f)\n",
    "\n",
    "train_doc2vec(tagged_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_tagged_documents(tagged_documents, filename='preprocessing_data/tagged_documents.json'):\n",
    "    tagged_documents_dict = [{\"tags\": doc.tags, \"words\": doc.words} for doc in tagged_documents]\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(tagged_documents_dict, f)\n",
    "\n",
    "save_tagged_documents(tagged_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Train Doc2Vec models\n",
    "model_dbow = Doc2Vec(tagged_documents, vector_size=75, window=10, min_count=2, workers=8, epochs=20, dm=0, alpha=0.03, min_alpha=0.0007, sample=1e-3)\n",
    "model_dm = Doc2Vec(tagged_documents, vector_size=75, window=10, min_count=2, workers=8, epochs=20, dm=1, alpha=0.03, min_alpha=0.0007, sample=1e-3)\n",
    "\n",
    "vectors_dbow = model_dbow.dv.vectors\n",
    "vectors_dm = model_dm.dv.vectors\n",
    "\n",
    "vectors_combined = np.hstack((vectors_dbow, vectors_dm))\n",
    "\n",
    "\n",
    "np.save('models/combined_vectors.npy', vectors_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import MmCorpus\n",
    "\n",
    "def flatten_texts(texts_words):\n",
    "    return [[word for sentence in text for word in sentence] for text in texts_words]\n",
    "\n",
    "def create_dictionary_corpus(flattened_texts_words):\n",
    "    dictionary = Dictionary(flattened_texts_words)\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.7)\n",
    "    corpus = [dictionary.doc2bow(text) for text in flattened_texts_words]\n",
    "    return dictionary, corpus\n",
    "\n",
    "def lda_topic_modeling(dictionary, corpus, num_topics=15, passes=25):\n",
    "    lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=passes)\n",
    "    topics = lda.print_topics(num_words=20)\n",
    "    return topics, lda\n",
    "\n",
    "def topic_modeling_pipeline(texts_words):\n",
    "    flattened_texts_words = flatten_texts(texts_words)\n",
    "    dictionary, corpus = create_dictionary_corpus(flattened_texts_words)\n",
    "    # Perform LDA topic modeling\n",
    "    topics, lda = lda_topic_modeling(dictionary, corpus)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "    lda.save(\"models/lda_model\")  \n",
    "    dictionary.save(\"models/dictionary\")\n",
    "    MmCorpus.serialize(\"models/corpus.mm\", corpus)\n",
    "\n",
    "topic_modeling_pipeline(lower_texts_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lattice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
